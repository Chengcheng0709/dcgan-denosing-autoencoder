<!DOCTYPE html>
<html>
<head>
<title>Generative Adversarial Denoising Autoencoder for Face Completion</title>
<script src="https://ajax.googleapis.com/ajax/libs/jquery/1.8.3/jquery.min.js"></script>
<script src="http://code.jquery.com/ui/1.9.2/jquery-ui.js"></script>

<script>
$(function(){
  $("#denoising_one_encoding_mse_test_vs_train").load("plots/denoising_one_encoding_mse_test_vs_train.html");
});
</script>

<script>
$(function(){
  $("#denoising_one_encoding_discrim_testing").load("plots/denoising_one_encoding_discrim_testing.html");
  });
</script>

<script>
$(function(){
  $("#autoencoder_mse_test_vs_train").load("plots/autoencoder_mse_test_vs_train.html");
});
</script>

<script>
$(function(){
  $("#denoising_one_hundreth_encoding_mse_test_vs_train").load("plots/denoising_one_hundreth_encoding_mse_test_vs_train.html");
});
</script>

<script>
$(function(){
  $("#denoising_one_hundreth_encoding_discrim_testing").load("plots/denoising_one_hundreth_encoding_discrim_testing.html");
});
</script>

<script>
$(function(){
  $("#denoising_one_encoding_nopooling_discrim_testing").load("plots/denoising_one_encoding_mse_test_vs_train.html");
});
</script>

<link rel="stylesheet" type="text/css" href="style.css">
</head>

<body>
  <div id="primarycontent">
    <center><h1>Generative Adversarial Denoising Autoencoder for Face Completion</h1></center>
      <center><ul id="people">
      <li><a href="">Avery Allen, </a><a href="http://wenchenli.github.io/about/">Wenchen Li </a></li>
    </ul></center>
    <h2>Project Overview</h2>
    <p>
        Our original project focus was creating a pipeline for photo restoration of portrait images.  Many damaged photos are available online, and current photo restoration solutions either provide unsatisfactory results, or
        require an advanced understanding of image editing software.  Shortly after begining the project, we narrowed our scope to what we consider the most interesting subproblem of photo resotration: facial image completion.
        The image completion problem we attempted to solve was as follows, given an image of a face with a square section of the image set to be white, fill in the missing pixels.
    </p>
        <figure>
          <center><img src="images/training_imgs.jpg" alt="Training Images"></center>
          <center><figcaption>Original image and masked image.</figcaption></center>
        </figure>
    <p>
        There are previous works that attempt to solve similar problems.  <a href = "http://ieeexplore.ieee.org/xpl/login.jsp?tp=&arnumber=5705573&url=http%3A%2F%2Fieeexplore.ieee.org%2Fxpls%2Fabs_all.jsp%3Farnumber%3D5705573">
        Graph Laplace for Occluded Face Completion and Recognition</a> and <a href = "http://ieeexplore.ieee.org/xpl/login.jsp?tp=&arnumber=5413474&url=http%3A%2F%2Fieeexplore.ieee.org%2Fxpls%2Fabs_all.jsp%3Farnumber%3D5413474">
        Partially occluded face completion and recognition</a> both leverage a large image database to find similar faces to use to complete the missing patch, but results are only shown for low resolution grey scale images.
        <a href = "https://papers.nips.cc/paper/4686-image-denoising-and-inpainting-with-deep-neural-networks.pdf"> Image Denoising and Inpainting with Deep Neural Networks</a> uses deep networks pre-trained with denoising autoencoders for image inpainting,
        but do no show completion of large missing image patches.
    </p>
    <p>
        Our approach builds upon the work presented in <a href = "http://arxiv.org/pdf/1511.06434v2.pdf"> Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks</a> and
        <a href = "https://swarbrickjones.wordpress.com/2016/01/13/enhancing-images-using-deep-convolutional-generative-adversarial-networks-dcgans/">Enhancing images using Deep Convolutional Generative Adversarial Networks (DCGANs)</a>.  In our approach we use a generator,
        a collection of convolution and deconvolution layers, to reconstruct the original unmasked image.  A discriminator is also trained using the output of the generator.  The discriminator's output is used together with a reconstruction cost to update the weights of the generator.

    </p>

    <h2>Dataset</h2>
    <p>
        We use the <a href= "http://mmlab.ie.cuhk.edu.hk/projects/CelebA.html"> CelebA Dataset </a> to train our model.  The dataset consists of around 200,000 images including over 10,000 unique identites.
        We reserve 1000 images as a test set, and 1000 images as a validation set.  For each image, we set a randomly sized patch to be white.  Each generated patch is at least 10 pixels in width/height and
        takes up a maximum of 1/3 of the image.  The patch's X,Y coordinates are selected randomly.
    </p>

    <h2>Architecture</h2>
    <h3>Overview</h3>
    The architecture has two major components: the autoencoder, and the discriminator.  A diagram of the architecture is shown below.  The autoencoder (left side of diagram) accepts a masked image as an input, and
    attempts to reconstruct the original unmasked image.  The discriminator (right side) is trained to determine whether a given image is a face.  The discriminator is run using the output of the autoencoder.  The result is used to
    influence the cost function used to update the autoencoder's weights.  We built upon <a href = "https://swarbrickjones.wordpress.com/2016/01/13/enhancing-images-using-deep-convolutional-generative-adversarial-networks-dcgans/">Enhancing images using Deep Convolutional
        Generative Adversarial Networks (DCGANs)</a>'s codebase.  The project code can be found in this <a href = "https://github.com/averyallen71/dcgan-denosing-autoencoder">repository</a>.
    <figure>
      <center><img src="images/architecture_overview.png" alt="Architecture Overview" width="800"></center>
    </figure>
    <h3>Autoencoder</h3>
    Traditionally, autoencoders have been used to learn a feature representation for some data set.  Denoising autoencoders artificially corrupt input data in order to force a more robust representation to be learned.
    In our case, the image mask is the data corruption.  Our autoencoder first transforms the input data through a series of 4 convolution layers.  At the center layer of the autoencoder, the image is represented by 1024 8 x 10
    feature maps.  The input image is then reconstructed by 4 fractionally strided convolution layers (sometimes called deconvolution layers).  The output layer uses a tanh activation function.  The autoencoder's weights are updated by a combination of two cost: the mean squared error
    between the original unmasked image and the reconsted image, and binary cross entropy using the output of the discriminator (desribed in the next section).  A diagram of the autoencoder is shown below.
    <figure>
      <center><img src="images/architecture_autoencoder.png" alt="Autoencoder Architecture" width="800"></center>
    </figure>
    <h3>Discriminator</h3>
    The discriminator consists of 4 convolutional layers.  It accepts a 128x160 RGB image as input.  The discriminator is trained to determine whether the input image is a real face.  A sigmoid function is used on the final layer to yield a probability between 0 and 1.  The probability represents
    the discriminator's belief that the input image is a real face.  During training, the discriminator is updated using real images from the training set, and images generated from the autoencoder.  The discriminator is rewarded for assigning a low probability to generated faces, and a high probability
    to real faces.  The cost functions are described in the next section.  A diagram of the discriminator is shown below.
    <figure>
      <center><img src="images/architecture_discriminator.png" alt="Discriminator Architecture" width="800"></center>
    </figure>


    <p>

    <h3>DCGAN Guidelines</h3>
    The architecture adheres to the following guidelines suggested in <a href = "http://arxiv.org/pdf/1511.06434v2.pdf"> Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks</a> which seemed to help make training stable:
    <!-- <figure>
      <center><img src="images/dcgan_guidelines.png" alt="DCGAN Guidelines"></center>
    </figure> -->
    <p>
      <ul>
        <li>Replace any pooling layers with strided convolutions (discriminator) and fractional-strided
        convolutions (generator).</li>
        <li>Use batchnorm in both the generator and the discriminator.</li>
        <li>Remove fully connected hidden layers for deeper architectures.</li>
        <li>Use ReLU activation in generator for all layers except for the output, which uses Tanh.</li>
        <li>Use LeakyReLU activation in the discriminator for all layers.</li>
      </ul>
    </p>

    </p>


    <h2>Loss functions</h2>
    There are four loss values we define in our model:
    <xmp>
      d_cost_real = binary_cross_entropy(p_real, T.ones(p_real.shape)).mean()
      d_cost_gen = binary_cross_entropy(p_gen, T.zeros(p_gen.shape)).mean()

      g_cost_d = binary_cross_entropy(p_gen, T.ones(p_gen.shape)).mean()
      enc_cost = mse(source_flat, target_flat).mean()
    </xmp>
      <p>The autoencoder's ability to fool the discriminator is determined by the discriminator's response when given a generated face (how well
        p_gen approximates p_real, g_cost_d).  P_gen and p_real are both probability distributions from the sigmoid function at the last layer of the discriminator. The autoencoder is also forced to learn the pixelwise representation of the faces by learning from the mean square errors from
        the groundtruth images(enc_cost).
      </p>
      <p>The discriminator's performance is measured by how well it can identify faces from groundtruth images(d_cost_real), and also how well it can
        recognize faces generated from the autoencoder(d_cost_gen).
      </p>

      <p>
          The cost function used to update the autoencoder is:
          <xmp>
              g_cost_d + enc_cost / X
          </xmp> X is a hyper parameter that decides how heavily to weight MSE when calculating the autoencoder cost.
          The cost function used to update the discriminator is:
          <xmp>
              d_cost_real + d_cost_gen
          </xmp>
      </p>


    During training and testing, we track: g_cost_d, enc_cost and d_cost_real.
    <h2>Results</h2>
    <center><img src="images/one.png"></center>

    <center>The base setup is the denoising autoencoder and discriminator with x = 1.</center>

      <h3>Base Setup Testing Set Results:</h3>
      Results from images in the 1000 image testing set.
          <h4><center>Test results along training epoches</center></h4>
            <center>
              <iframe width="320" height="180" src="https://www.youtube.com/embed/SXWT4DDnukI" frameborder="0" allowfullscreen></iframe>
              <iframe width="320" height="180" src="https://www.youtube.com/embed/3iqZ5s_2HoE" frameborder="0" allowfullscreen></iframe>
            </center>
          <h4><center>final test results</center></h4>
            <center>
              <iframe width="560" height="315" src="https://www.youtube.com/embed/YAv9K4U39oc" frameborder="0" allowfullscreen></iframe>
            </center>
          <h4><center>loss values along epoches</center></h4>
          <div id="denoising_one_encoding_mse_test_vs_train"></div>
          <div id="denoising_one_encoding_discrim_testing"></div>
          <p>
            Around 10k iterations, the encoding cost reaches a local minimum. We see a divergence trend starting
            at around 15k between the autoencoder's ability to fool the discriminator (g_cost_d) and the discriminator's ability of
            identifying real face data(d_cost_real).
          </p>
          <p>
            A possible explanation is that as the MSE encoding cost converges on a lower bound, the autoencoder is no longer to fool the discriminator by updating with a loss function that so heavily favors the already converged MSE.
            A possible solution would be to use a dynamic learning mechanism to adjust the cost function for the autoencoder.
            As training progresses, the hyperparameter X should be updated to weight MSE loss less once MSE cost has converged because learning from MSE no longer seems to be
            "efficient" in the sense of fooling the discriminator.
          </p>

    <h2>Experiments</h2>
      <p>To help better understand how the autoencoder and discriminator work together, we ran several control experiments based on our initial setup.

      <h3>Without Discriminator</h3>

      <center><img src="images/noDiscriminator.png"></center>
        <h4><center>without discriminator mse loss</center></h4>
          <div id="autoencoder_mse_test_vs_train"></div>
          <p>
            We see that the discriminator does help with reconstruction even though it's objective is to discriminate between real face data and generated face data.
          </p>

      <h3>Decrease Mean Squared Error's Infulence on Reconstruction</h3>
      <center><img src="images/one_hundred.png"></center>
      <!-- <h4><center>mse loss without pooling</center></h4> -->
        <div id="denoising_one_hundreth_encoding_mse_test_vs_train"></div>
        <div id="denoising_one_hundreth_encoding_discrim_testing"></div>
        <p>
          To experiment with how to combine MSE loss and discriminator loss for autoencoder updates, we set
          <xmp>
            generator_loss = MSE * X + g_cost_d
          </xmp>
          where X = .01(base model X = 1).
          From the X = .01 experiment, we see reconstruction loss reach a local minimum at a loss value much higher than X = 1.
          This leads the generator to learn more from the discriminator and reach an equilibrium quickly, but leads
          to slightly worse reconstruction of the original input.  We believe that generated and real image discriminator loss reaching the equilibrium point too early leads
          the discriminator and autoencoder to stop learning from one another. This is because both sides already believe they are doing a good job based on their objective.
          As mentioned before, we believe that adding a way to actively adjust the X hyperparameter may lead to more stable training.
        </p>

      <h3>MSE Without Pooling</h3>
      <center><img src="images/nopoolingmse.png"></center>
        <h4><center>MSE Without Pooling</center></h4>
          <div id="denoising_one_encoding_nopooling_discrim_testing"></div>
          <p>
            The original implementation provided by <a href = "https://swarbrickjones.wordpress.com/2016/01/13/enhancing-images-using-deep-convolutional-generative-adversarial-networks-dcgans/">Enhancing images using Deep Convolutional Generative Adversarial Networks (DCGANs)</a>
            performs the additional step of max pooling on the reconstructed and target image before calculating the mean squared error.  Our base implementation also uses this method.  The reason described for including max pooling is that it counteracts the blurred look
            that is often present when using denoising autoencoders.  We trained an additional model to test whether pooling was effectively preventing blur.  The results above clearly show that removing pooling does in fact bring back a blurred look to the problem area.  However,
            the areas of the image that were not masked in the input image seem to be mostly clear of artifacts.  This is in direct contrast from when pooling is used; images often have artifacts on areas outside of the original image mask.
          </p>
  </div>

</body>

</html>
